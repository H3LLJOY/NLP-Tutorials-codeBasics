{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-12.png)\n",
    "![alt text](image-13.png)\n",
    "![alt text](image-14.png)\n",
    "\n",
    "For Stemming we will use NLTK becouse StaCy does'nt support stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating -> eat\n",
      "eats -> eat\n",
      "eat -> eat\n",
      "ate -> ate\n",
      "adjustable -> adjust\n",
      "rafting -> raft\n",
      "abillity -> abil\n",
      "meeting -> meet\n",
      "better -> better\n"
     ]
    }
   ],
   "source": [
    "words = ['eating', 'eats', 'eat', 'ate','adjustable','rafting','abillity','meeting' ,'better']\n",
    "\n",
    "for word in words:\n",
    "    print(word + ' -> ' + stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating -> eat -> 9837207709914848172\n",
      "eats -> eat -> 9837207709914848172\n",
      "eat -> eat -> 9837207709914848172\n",
      "ate -> eat -> 9837207709914848172\n",
      "adjustable -> adjustable -> 6033511944150694480\n",
      "rafting -> raft -> 7154368781129989833\n",
      "abillity -> abillity -> 7770879582343626259\n",
      "meeting -> meet -> 6880656908171229526\n",
      "better -> well -> 4525988469032889948\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"eating eats eat ate adjustable rafting abillity meeting better\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text + ' -> ' + token.lemma_ ,'->',token.lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ability -> ability this particular model එක train කරලා තියෙන්නේ මේ විදියට.අපිට පුලුවන් අවශ්‍ය ඒවා customize කරගන්න."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By attribute ruler we can  Fine-tune or override default behavior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro -> Bro\n",
      ", -> ,\n",
      "you -> you\n",
      "wanna -> wanna\n",
      "go -> go\n",
      "? -> ?\n",
      "Brah -> Brah\n",
      ", -> ,\n",
      "do -> do\n",
      "n't -> not\n",
      "say -> say\n",
      "no -> no\n",
      "! -> !\n",
      "I -> I\n",
      "am -> be\n",
      "exhausted -> exhaust\n",
      ". -> .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text + ' -> ' + token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = nlp.get_pipe(\"attribute_ruler\")\n",
    "ar.add([[{\"Text\":\"Bro\"}], [{\"Text\":\"Brah\"}]],{\"Lemma\":'brother'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro -> brother\n",
      ", -> ,\n",
      "you -> you\n",
      "wanna -> wanna\n",
      "go -> go\n",
      "? -> ?\n",
      "Brah -> brother\n",
      ", -> ,\n",
      "do -> do\n",
      "n't -> not\n",
      "say -> say\n",
      "no -> no\n",
      "! -> !\n",
      "I -> I\n",
      "am -> be\n",
      "exhausted -> exhaust\n",
      ". -> .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text + ' -> ' + token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise:\n",
    "\n",
    "> convert the given text into it's base form using both stemming and lemmatization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Latha is very multi talented girl.She is good at many skills like dancing, running, singing, playing.She also likes eating Pav Bhagi. she has a \n",
    "habit of fishing and swimming too.Besides all this, she is a wonderful at cooking too.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let import necessary libraries and create the object\n",
    "\n",
    "#for nltk\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#downloading all neccessary packages related to nltk\n",
    "nltk.download('all')\n",
    "\n",
    "\n",
    "#for spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latha -> latha\n",
      "is -> is\n",
      "very -> veri\n",
      "multi -> multi\n",
      "talented -> talent\n",
      "girl.She -> girl.sh\n",
      "is -> is\n",
      "good -> good\n",
      "at -> at\n",
      "many -> mani\n",
      "skills -> skill\n",
      "like -> like\n",
      "dancing -> danc\n",
      ", -> ,\n",
      "running -> run\n",
      ", -> ,\n",
      "singing -> sing\n",
      ", -> ,\n",
      "playing.She -> playing.sh\n",
      "also -> also\n",
      "likes -> like\n",
      "eating -> eat\n",
      "Pav -> pav\n",
      "Bhagi -> bhagi\n",
      ". -> .\n",
      "she -> she\n",
      "has -> ha\n",
      "a -> a\n",
      "habit -> habit\n",
      "of -> of\n",
      "fishing -> fish\n",
      "and -> and\n",
      "swimming -> swim\n",
      "too.Besides -> too.besid\n",
      "all -> all\n",
      "this -> thi\n",
      ", -> ,\n",
      "she -> she\n",
      "is -> is\n",
      "a -> a\n",
      "wonderful -> wonder\n",
      "at -> at\n",
      "cooking -> cook\n",
      "too -> too\n",
      ". -> .\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk_words = word_tokenize(text)\n",
    "\n",
    "for word in nltk_words:\n",
    "    print(word + ' -> ' + stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latha -> Latha\n",
      "is -> be\n",
      "very -> very\n",
      "multi -> multi\n",
      "talented -> talented\n",
      "girl -> girl\n",
      ". -> .\n",
      "She -> she\n",
      "is -> be\n",
      "good -> good\n",
      "at -> at\n",
      "many -> many\n",
      "skills -> skill\n",
      "like -> like\n",
      "dancing -> dancing\n",
      ", -> ,\n",
      "running -> running\n",
      ", -> ,\n",
      "singing -> singing\n",
      ", -> ,\n",
      "playing -> play\n",
      ". -> .\n",
      "She -> she\n",
      "also -> also\n",
      "likes -> like\n",
      "eating -> eat\n",
      "Pav -> Pav\n",
      "Bhagi -> Bhagi\n",
      ". -> .\n",
      "she -> she\n",
      "has -> have\n",
      "a -> a\n",
      "\n",
      " -> \n",
      "\n",
      "habit -> habit\n",
      "of -> of\n",
      "fishing -> fishing\n",
      "and -> and\n",
      "swimming -> swim\n",
      "too -> too\n",
      ". -> .\n",
      "Besides -> besides\n",
      "all -> all\n",
      "this -> this\n",
      ", -> ,\n",
      "she -> she\n",
      "is -> be\n",
      "a -> a\n",
      "wonderful -> wonderful\n",
      "at -> at\n",
      "cooking -> cook\n",
      "too -> too\n",
      ". -> .\n",
      "\n",
      " -> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text + ' -> ' + token.lemma_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
